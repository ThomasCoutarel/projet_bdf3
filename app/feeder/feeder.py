from pyspark.sql import SparkSession
import os

# Configuration des chemins
class Config:
    # Chemin local pour Temp (donn√©es du jour uniquement)
    TEMP_PATH = "/app/bronze_data/temp"
    # Chemin HDFS pour Bronze (donn√©es cumul√©es)
    BRONZE_ROOT = "hdfs://namenode:8020/lakehouse/bronze"

    @staticmethod
    def get_temp_dates():
        """
        Liste les dates disponibles dans TEMP_PATH
        """
        try:
            return sorted(os.listdir(Config.TEMP_PATH))
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Dossier temp non trouv√© : {Config.TEMP_PATH}")
            return []

    @staticmethod
    def get_last_bronze_date():
        """
        Liste les dates disponibles dans Bronze (HDFS)
        """
        # Ici on va utiliser spark.read pour v√©rifier la pr√©sence car os.listdir ne fonctionne pas sur HDFS localement
        # Donc on listera via Spark (plus fiable)
        from py4j.protocol import Py4JJavaError
        try:
            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
            path = spark._jvm.org.apache.hadoop.fs.Path(Config.BRONZE_ROOT)
            if not fs.exists(path):
                return None
            statuses = fs.listStatus(path)
            # Extraire les noms des dossiers (dates)
            dates = [status.getPath().getName() for status in statuses if status.isDirectory()]
            return sorted(dates)[-1] if dates else None
        except Py4JJavaError as e:
            print(f"Erreur en listant les dossiers bronze HDFS : {e}")
            return None

    @staticmethod
    def get_parquet_path(base, date_str):
        """
        Construit le chemin parquet pour une date donn√©e
        """
        return os.path.join(base, date_str, "parquet")


# Initialisation Spark
spark = SparkSession.builder \
    .appName("Feeder Optimis√© Incr√©mental") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

print("üöÄ Spark pr√™t")

# √âtape 1 : identifier la derni√®re date Bronze cumul√©e
last_bronze_date = Config.get_last_bronze_date()
print(f"üì¶ Dernier bronze trouv√© : {last_bronze_date}")

# √âtape 2 : lister les dates Temp disponibles (local)
all_temp_dates = Config.get_temp_dates()
print(f"üìÖ Dates temp disponibles : {all_temp_dates}")

# √âtape 3 : d√©terminer les dates √† traiter (sup√©rieures √† la derni√®re date Bronze)
dates_to_process = [d for d in all_temp_dates if (last_bronze_date is None or d > last_bronze_date)]
print(f"üÜï Nouvelles dates √† traiter : {dates_to_process}")

# √âtape 4 : boucle de traitement incr√©mental
for new_date in dates_to_process:
    print(f"\n‚è≥ Traitement du jour : {new_date}")

    path_today = Config.get_parquet_path(Config.TEMP_PATH, new_date)
    path_today = f"file://{path_today}"  # pr√©ciser protocole local

    # Lecture Temp locale
    df_today = spark.read.parquet(path_today)
    print(f"üìÅ {df_today.count()} lignes lues depuis {path_today}")

    if last_bronze_date:
        path_bronze = Config.get_parquet_path(Config.BRONZE_ROOT, last_bronze_date)
        path_bronze = f"{path_bronze}/parquet" if not path_bronze.endswith("parquet") else path_bronze
        try:
            # Lecture Bronze cumul√©e pr√©c√©dente depuis HDFS
            df_prev = spark.read.parquet(path_bronze)
            print(f"üìÅ {df_prev.count()} lignes dans le bronze pr√©c√©dent : {last_bronze_date}")
            # Union incr√©mentale
            accumulated = df_prev.union(df_today)
        except Exception as e:
            print(f"‚ö†Ô∏è Bronze pr√©c√©dent non trouv√© ou erreur lecture : {e}")
            print("üÜï Initialisation Bronze avec la date courante uniquement.")
            accumulated = df_today
    else:
        # Premier traitement, on initialise le Bronze avec les donn√©es du jour
        accumulated = df_today

    # R√©partition (optionnelle, selon colonne "State")
    if "State" in accumulated.columns:
        accumulated = accumulated.repartition(10, "State")
    else:
        accumulated = accumulated.repartition(10)

    # √âcriture Bronze cumul√©e dans HDFS
    new_bronze_path = Config.get_parquet_path(Config.BRONZE_ROOT, new_date)
    print(f"üíæ Sauvegarde Bronze dans HDFS : {new_bronze_path}")
    accumulated.coalesce(1).write.mode("overwrite").parquet(new_bronze_path)

    print(f"‚úÖ Bronze mis √† jour pour {new_date} : {accumulated.count()} lignes")

    # Mise √† jour de la derni√®re date Bronze
    last_bronze_date = new_date


spark.stop()
print("üî• Traitement termin√©")
